file <- 1
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
#idx <- substr(links,1,1) == '#'  # index those beginning with # - internal links
links2 <- setdiff(links,'#') # setdiff is set union this gets those items not in idx
write.csv(links, file='d:/data/dir/links.csv')
write.csv(links, file='d:/data/dir/links2.csv')
rm(list=ls())
library("RCurl")
library(XML)
dir="d:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
file <- 1
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
#idx <- substr(links,1,1) == '#'  # index those beginning with # - internal links
links2 <- setdiff(links,'#') # setdiff is set union this gets those items not in idx
write.csv(links, file='e:/data/dir/links.csv')
write.csv(links2, file='e:/data/dir/links2.csv')
rm(list=ls())
library("RCurl")
library(XML)
dir="d:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
file <- 1
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is a set difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
links
test <- writeurltext(links[1])
filename <- paste(baseurl,links[1], sep="")
test <- writeurltext(links[1], filname)
filename <- paste(baseurl,links[1], sep="")
test <- writeurltext(links[1], filename)
links[1]
as.char(links[1])
as.character(links[1])
rm(list=ls())
library("RCurl")
library(XML)
dir="d:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
url
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is a set difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
filename <- paste(baseurl,links[1], sep="")
test <- writeurltext(links[1], filename)
as.character(links[1])
class(links)
str(links)
rm(list=ls())
library("RCurl")
library(XML)
dir="d:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
url
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is a set difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
filename <- paste(baseurl,links[1], sep="")
url <- links[1]
test <- writeurltext(url, filename)
class(links)
str(links)
filename <- paste(baseurl,links[1], sep="")
url <- paste(links[1], 'index.asp', sep="")links[1]
test <- writeurltext(url, filename)
filename <- paste(baseurl,links[1], sep="")
url <- paste(links[1], '/index.asp', sep="")
test <- writeurltext(url, filename)
url
i=1
filename <- paste(i,'links[1]'.txt', sep="")
url <- paste(links[1], '/index.asp', sep="")
test <- writeurltext(url, filename)
url
i=1
filename <- paste(i,'.txt', sep="")
url <- paste(links[1], '/index.asp', sep="")
test <- writeurltext(url, filename)
i=1
filename <- paste(i,'.txt', sep="")
filename <- paste(dir,filename,sep='')
url <- paste(links[1], '/index.asp', sep="")
test <- writeurltext(url, filename)
rm(list=ls())
library("RCurl")
library(XML)
dir="d:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
url
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is a set difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
i=1
filename <- paste(i,'.txt', sep="")
filename <- paste(dir,filename,sep='')
url <- paste(links[1], '/index.asp', sep="")
test <- writeurltext(url, filename)
url
rm(list=ls())
library("RCurl")
library(XML)
dir="e:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
url
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is a set difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
i=1
filename <- paste(i,'.txt', sep="")
filename <- paste(dir,filename,sep='')
url <- paste(links[1], '/index.asp', sep="")
test <- writeurltext(url, filename)
url
class(links)
str(links)
rm(list=ls())
library("RCurl")
library(XML)
dir="e:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
url
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is a set difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
len=length(links
for (i in 1:len) {
filename <- paste(i,'.txt', sep="")
filename <- paste(dir,filename,sep='')
url <- paste(links[1], '/index.asp', sep="")
test <- writeurltext(url, filename)
}
len=length(links)
for (i in 1:len) {
filename <- paste(i,'.txt', sep="")
filename <- paste(dir,filename,sep='')
url <- paste(links[1], '/index.asp', sep="")
test <- writeurltext(url, filename)
}
rm(list=ls())
library("RCurl")
library(XML)
dir="e:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
url
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is a set difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
len=length(links)
for (i in 1:5) {
filename <- paste(i,'.txt', sep="")
filename <- paste(dir,filename,sep='')
url <- paste(links[i], '/index.asp', sep="")
test <- writeurltext(url, filename)
}
links
len=length(links)
for (i in 2:5) {
filename <- paste(i,'.txt', sep="")
filename <- paste(dir,filename,sep='')
url <- paste(links[i], '/index.asp', sep="")
test <- writeurltext(url, filename)
}
length baseurl
length(baseurl)
rm(list=ls())
library("RCurl")
library(XML)
dir="e:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
url
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is a set difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
# lose those that aren't birmingham
idx <- substr(links,1,28) == 'http://www.birmingham.ac.uk/'
links <- setdiff(links, idx)
links
substr(links,1,28) == 'http://www.birmingham.ac.uk/'
rm(list=ls())
library("RCurl")
library(XML)
dir="e:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
url
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
# lose those that aren't birmingham
idx <- substr(links,1,28) == 'http://www.birmingham.ac.uk/'
links <- links(idx)
links[idx]
rm(list=ls())
library("RCurl")
library(XML)
dir="e:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
url
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
# lose those that aren't birmingham
idx <- substr(links,1,28) == 'http://www.birmingham.ac.uk/'
links <- links[idx]
for (i in 2:5) {
filename <- paste(i,'.txt', sep="")
filename <- paste(dir,filename,sep='')
url <- paste(links[i], '/index.asp', sep="")
test <- writeurltext(url, filename)
}
rm(list=ls())
library("RCurl")
library(XML)
dir="e:/data/dir/"
baseurl <- "http://www.birmingham.ac.uk"
url <- paste(baseurl,"/index.aspx", sep="")
######
# function to read a webpage and write a file for each link on the page
######
writeurltext <- function(url, filename) {
url
html <- getURL(url, followlocation = TRUE)
html <- htmlParse(html, useInternal = TRUE)
links <- as.vector(xpathSApply(html, "//a/@href")) # as.vector gets rid of the href attribute
# parse html
txt <- xpathApply(html, "//body//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)]", xmlValue)
#file <- paste(dir,as.character(file),'.txt',sep="")
sink(filename)
cat(paste(txt), collapse = " ")
sink()
return(links)
}
# test <- writeurltext(baseurl,"E:/data/dir/text.txt")
# Get links from base page
html <- getURL(url, followlocation = TRUE) # reads the main page
html <- htmlParse(html, useInternal = TRUE) # parses the html
links <- as.vector(xpathSApply(html, "//a/@href")) # gets the links, as.vector gets rid of the href attribute
links <- setdiff(links,'#') # setdiff is difference from links and '#' - so removes #'s
idx <- substr(links,1,1) == '/' # build an index list of those which are a subpage, I need the full url
links[idx] <- paste(baseurl,links[idx],sep="") # apply the concatenation to those items in the index
# lose those that aren't birmingham
idx <- substr(links,1,28) == 'http://www.birmingham.ac.uk/'
links <- links[idx]
len=length(links)
for (i in 1:len) {
filename <- paste(i,'.txt', sep="")
filename <- paste(dir,filename,sep='')
url <- paste(links[i], '/index.asp', sep="")
test <- writeurltext(url, filename)
}
home <- F
dir="d:/data/dir"
if (home) {
dir="e:/data/dir/" }
home <- T
dir="d:/data/dir"
if (home) {
dir="e:/data/dir/" }
home <- T
laptop = F
dir="d:/data/dir"
if (home) dir="e:/data/dir/"
if (laptop) dir <- "c:/data/dir"
